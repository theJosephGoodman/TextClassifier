{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d2d0e7-fb56-4468-a8a4-62b36ffa79d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30d2d0e7-fb56-4468-a8a4-62b36ffa79d2",
    "outputId": "43468173-07e1-47ca-e68a-c1acf1a2321a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: catboost in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: graphviz in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (0.19.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (1.20.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (5.6.0)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (3.3.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (1.6.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catboost) (1.2.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "!pip install pymorphy2;\n",
    "import pymorphy2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "!pip install transformers;\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "!pip install catboost;\n",
    "import catboost\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95de695c-3abd-4e8c-a395-aec28bd3bdfb",
   "metadata": {
    "id": "95de695c-3abd-4e8c-a395-aec28bd3bdfb"
   },
   "outputs": [],
   "source": [
    "set_stopwords = stopwords.words('russian')\n",
    "analyzer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "train_orig = pd.read_csv('../data/train.tsv', sep='\\t')\n",
    "test_orig = pd.read_csv('../data/test.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc656b1a-52d1-4445-b4e3-268ae333558e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dc656b1a-52d1-4445-b4e3-268ae333558e",
    "outputId": "275a0b99-53be-4376-9d9a-c28c6288e180"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Москвичу Владимиру Клутину пришёл счёт за вмеш...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Агент Кокорина назвал езду по встречке житейск...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Госдума рассмотрит возможность введения секрет...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ФАС заблокировала поставку скоростных трамваев...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Против Навального завели дело о недоносительст...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_fake\n",
       "0  Москвичу Владимиру Клутину пришёл счёт за вмеш...        1\n",
       "1  Агент Кокорина назвал езду по встречке житейск...        0\n",
       "2  Госдума рассмотрит возможность введения секрет...        1\n",
       "3  ФАС заблокировала поставку скоростных трамваев...        0\n",
       "4  Против Навального завели дело о недоносительст...        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be08a4c4-f553-47ea-9732-f256abf20461",
   "metadata": {
    "id": "be08a4c4-f553-47ea-9732-f256abf20461"
   },
   "outputs": [],
   "source": [
    "train = train_orig.copy()\n",
    "test = test_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729cd600-42dc-4249-a5df-6086d083b356",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "729cd600-42dc-4249-a5df-6086d083b356",
    "outputId": "317fe950-7086-4f8b-c79a-20b84ec21dfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "is_fake    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049a7574-e78a-4889-b711-7610444c0890",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "049a7574-e78a-4889-b711-7610444c0890",
    "outputId": "29683f6c-fc3d-4d12-c97f-65f240ec5de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность тренировочных данных: (5758, 2)\n",
      "Размерность тестовых данных: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'Размерность тренировочных данных: {train.shape}')\n",
    "print(f'Размерность тестовых данных: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc8f9f0-3db6-4055-81c7-22dbdf9f9ec8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbc8f9f0-3db6-4055-81c7-22dbdf9f9ec8",
    "outputId": "1d5c90f5-0386-476d-9f0e-9728b2fa834a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: is_fake, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# классов у нас по ровну - дисбаланса нету)\n",
    "train.is_fake.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd67b14-ee79-4066-b369-158ddc39cab9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebd67b14-ee79-4066-b369-158ddc39cab9",
    "outputId": "c29c2cbf-8058-4b68-a0cd-d78afbc82677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество слов до удаления символов и стоп слов:\n",
      "50237\n",
      "Количество слов после удаления символов и стоп слов:\n",
      "37617\n"
     ]
    }
   ],
   "source": [
    "print('Количество слов до удаления символов и стоп слов:')\n",
    "print(train.title.str.split(' ').apply(lambda x: len(x)).sum())\n",
    "\n",
    "print('Количество слов после удаления символов и стоп слов:')\n",
    "# привожу к нижнему регистру, удаляю символы и стоп-слова\n",
    "x_filtered = train.title.str.lower().str.split(' ').apply(lambda x: [i for i in x if (i.isalpha() & ~(i in set_stopwords))])\n",
    "print(x_filtered.apply(lambda x: len(x)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0934c74a-3ba5-43e9-8348-b36ce17406af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0934c74a-3ba5-43e9-8348-b36ce17406af",
    "outputId": "4c614465-27ce-4572-915f-078e8ead1727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [москвич, владимир, клутина, прийти, счёт, вме...\n",
      "1    [агент, кокорин, назвать, езда, встречка, жите...\n",
      "2    [госдума, рассмотреть, возможность, введение, ...\n",
      "3    [фас, заблокировать, поставка, скоростной, тра...\n",
      "4    [против, навальный, завести, дело, недоносител...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# лемматизирую с помощью библиотеки pymorphy\n",
    "x_lemmatized = x_filtered.apply(lambda x: [(analyzer.parse(word)[0]).normal_form for word in x ])\n",
    "\n",
    "print(x_lemmatized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c2d7a6-fa61-4ebb-8928-af6937d398ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c2d7a6-fa61-4ebb-8928-af6937d398ff",
    "outputId": "988bec3a-c958-4e05-aee2-ab13b01f9546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина настоящего сообщения: 5.81\n",
      "Средняя длина фейкового сообщения: 7.26\n"
     ]
    }
   ],
   "source": [
    "print(f\"Средняя длина настоящего сообщения: {round(x_lemmatized[train.is_fake==0].apply(lambda x: len(x)).mean(), 2)}\")\n",
    "print(f\"Средняя длина фейкового сообщения: {round(x_lemmatized[train.is_fake==1].apply(lambda x: len(x)).mean(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad8123fa-9d6d-476b-9e71-4710bd6b6fbe",
   "metadata": {
    "id": "ad8123fa-9d6d-476b-9e71-4710bd6b6fbe"
   },
   "outputs": [],
   "source": [
    "top_fake_counter = Counter()\n",
    "top_true_counter = Counter()\n",
    "\n",
    "for row in x_lemmatized[train.is_fake==1]:\n",
    "    top_fake_counter.update(row)\n",
    "for row in x_lemmatized[train.is_fake==0]:\n",
    "    top_true_counter.update(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc037fd5-136d-4973-932f-ad769948b905",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc037fd5-136d-4973-932f-ad769948b905",
    "outputId": "7a7ad77a-5736-4ae2-a10c-c6ede9cfd91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('россия', 266), ('российский', 141), ('год', 133), ('запретить', 120), ('навальный', 115)]\n",
      "[('россия', 230), ('российский', 143), ('новый', 118), ('год', 96), ('сша', 88)]\n"
     ]
    }
   ],
   "source": [
    "print(top_fake_counter.most_common(5))\n",
    "print(top_true_counter.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d101f0-9194-4492-a4d9-c8a0cacaa026",
   "metadata": {
    "id": "c3d101f0-9194-4492-a4d9-c8a0cacaa026"
   },
   "source": [
    "Наиболее встречаемые слова в каждом классе приблизительно одни и те же, но заметно, что средние длины фейков и настоящих сообщений отличаются.\n",
    "Можно было бы статистически проверить гипотезу, что между классами средняя длина сообщения различается и в случае статистически значимых различий вручную добавить в качестве фичи длину сообщения, но я решил не усложнять "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914182dd-cb5c-4ffd-9ac5-003f64e1b086",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "914182dd-cb5c-4ffd-9ac5-003f64e1b086",
    "outputId": "5873db1b-052d-418b-f1e7-129936d215ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    москвич владимир клутина прийти счёт вмешатель...\n",
       "1    агент кокорин назвать езда встречка житейский ...\n",
       "2    госдума рассмотреть возможность введение секре...\n",
       "3    фас заблокировать поставка скоростной трамвай ...\n",
       "4    против навальный завести дело недоносительство...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_final = x_lemmatized.str.join(' ')\n",
    "x_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5214eb",
   "metadata": {
    "id": "7e5214eb"
   },
   "source": [
    "#### 1 попытка - tfidf + random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717087a-480f-4f0e-8215-a238a7713fdb",
   "metadata": {
    "id": "f717087a-480f-4f0e-8215-a238a7713fdb"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_final, train.is_fake, stratify=train.is_fake, test_size = 0.2, random_state = 0)\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "x_train, x_val = tfidf.fit_transform(x_train), tfidf.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208aeaf-7ff0-4eab-bee9-fc64ea6a3549",
   "metadata": {
    "id": "9208aeaf-7ff0-4eab-bee9-fc64ea6a3549",
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(x_train, y_train)\n",
    "y_pred = forest.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97ceff-c000-4be6-897a-b599fd1d1a5d",
   "metadata": {
    "id": "6c97ceff-c000-4be6-897a-b599fd1d1a5d",
    "outputId": "150a84c9-308a-48e5-c260-1a68387c4539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.93      0.81       576\n",
      "           1       0.90      0.62      0.73       576\n",
      "\n",
      "    accuracy                           0.78      1152\n",
      "   macro avg       0.80      0.78      0.77      1152\n",
      "weighted avg       0.80      0.78      0.77      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdeb04-c5f6-4eff-9c33-a24b16f0bde5",
   "metadata": {
    "id": "77bdeb04-c5f6-4eff-9c33-a24b16f0bde5"
   },
   "source": [
    "#### 2 попытка - tfidf + фича длины текста + random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebc521-506a-409a-b862-33ddc7b82ee8",
   "metadata": {
    "id": "02ebc521-506a-409a-b862-33ddc7b82ee8"
   },
   "outputs": [],
   "source": [
    "x_final_2 = pd.concat((x_final,\n",
    "           x_final.apply(lambda x: len(x))\n",
    "          ), axis=1\n",
    "         )\n",
    "x_final_2.columns = ['text', 'num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19ea0c-fbf3-4bba-9337-67f465c0baac",
   "metadata": {
    "id": "ab19ea0c-fbf3-4bba-9337-67f465c0baac"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_final_2, train.is_fake, stratify=train.is_fake, test_size = 0.2, random_state = 0)\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "\n",
    "# кодирую фичи через tfidf и сразу же конкатенирую с фичой \"количество слов\"\n",
    "x_train = np.concatenate((tfidf.fit_transform(x_train['text']).toarray(),\n",
    "                          x_train.num_words.values[:, np.newaxis]),\n",
    "                         axis=1)\n",
    "\n",
    "x_val = np.concatenate((tfidf.transform(x_val['text']).toarray(),\n",
    "                        x_val.num_words.values[:, np.newaxis]),\n",
    "                       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19672b-99d4-49d6-a1ca-646cfecdde70",
   "metadata": {
    "id": "8c19672b-99d4-49d6-a1ca-646cfecdde70"
   },
   "outputs": [],
   "source": [
    "forest2 = RandomForestClassifier(random_state=0)\n",
    "forest2.fit(x_train, y_train)\n",
    "y_pred = forest2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0e143-21a4-43d2-9d8c-2c9c8fb56d96",
   "metadata": {
    "id": "9ef0e143-21a4-43d2-9d8c-2c9c8fb56d96",
    "outputId": "a34072ea-a7bb-445b-bc97-5cf3258b0baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.89      0.82       576\n",
      "           1       0.87      0.70      0.78       576\n",
      "\n",
      "    accuracy                           0.80      1152\n",
      "   macro avg       0.81      0.80      0.80      1152\n",
      "weighted avg       0.81      0.80      0.80      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Всё-таки эта фича оказалась полезной )\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c75cc",
   "metadata": {
    "id": "660c75cc"
   },
   "source": [
    "#### 3 попытка - попробую использовать дистилрованную(уменьшенную) русскоязычную версию Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "-limOvhl9rhZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-limOvhl9rhZ",
    "outputId": "51031149-e9d5-4eff-bb5d-2005fa68e6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/bionic.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 94 not upgraded.\n",
      "Need to get 6,800 kB of archives.\n",
      "After this operation, 15.3 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 3.1.2 [6,800 kB]\n",
      "Fetched 6,800 kB in 1s (11.3 MB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 156214 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.1.2_amd64.deb ...\n",
      "Unpacking git-lfs (3.1.2) ...\n",
      "Setting up git-lfs (3.1.2) ...\n",
      "Git LFS initialized.\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "# на colab не работает git lfs\n",
    "# на stackoverflow нашел решение проблемы в этом коде\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "DA9VgyVe9-Nm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DA9VgyVe9-Nm",
    "outputId": "3d88978f-cdd2-4461-b42a-905ec459273f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "Cloning into 'rubert-tiny'...\n",
      "remote: Enumerating objects: 79, done.\u001b[K\n",
      "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
      "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
      "remote: Total 79 (delta 37), reused 0 (delta 0)\u001b[K\n",
      "Unpacking objects: 100% (79/79), done.\n"
     ]
    }
   ],
   "source": [
    "# скачал дистилрованную версию русскоязычного bert, который работает в разы быстрее\n",
    "!git lfs clone  https://huggingface.co/cointegrated/rubert-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7b2a4c3-088c-4210-9edc-b139d4705142",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7b2a4c3-088c-4210-9edc-b139d4705142",
    "outputId": "1d0a5204-523c-459c-9191-1e987f387c53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"rubert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9270c2c0-cdd2-456c-bd73-0c5eb7139bd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9270c2c0-cdd2-456c-bd73-0c5eb7139bd0",
    "outputId": "f0edfddd-96f7-4790-ef7f-4e86d79bf07d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312,)\n"
     ]
    }
   ],
   "source": [
    "# функция для получения эмбеддингов\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(embed_bert_cls('привет мир', model, tokenizer).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0aba7ef-ce2d-4bf9-b6a7-940fcb359818",
   "metadata": {
    "id": "f0aba7ef-ce2d-4bf9-b6a7-940fcb359818"
   },
   "outputs": [],
   "source": [
    "# получил эмбеддинги и сконкатенировал с фичой длины текста\n",
    "x_final_bert = [embed_bert_cls(sentence, model, tokenizer) for sentence in x_final]\n",
    "x_final_bert = np.vstack(x_final_bert)\n",
    "x_final_bert_end = np.hstack((x_final_bert,\n",
    "                              train.title.apply(lambda x: len(x)).values[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b184acde-5c21-4a0c-a4f7-b7ae7a3dd43a",
   "metadata": {
    "id": "b184acde-5c21-4a0c-a4f7-b7ae7a3dd43a"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_final_bert_end, train.is_fake, random_state = 0, stratify = train.is_fake, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "accae45c-3bfb-4342-8b14-34abe1c20d61",
   "metadata": {
    "id": "accae45c-3bfb-4342-8b14-34abe1c20d61"
   },
   "outputs": [],
   "source": [
    "forest3 = RandomForestClassifier(random_state=0)\n",
    "forest3.fit(x_train, y_train)\n",
    "y_pred = forest3.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e46a5b0a-c710-42d8-abc7-240c073e0185",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e46a5b0a-c710-42d8-abc7-240c073e0185",
    "outputId": "3c080361-438a-41d7-9af1-2fc7cb560a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       576\n",
      "           1       0.81      0.76      0.79       576\n",
      "\n",
      "    accuracy                           0.79      1152\n",
      "   macro avg       0.79      0.79      0.79      1152\n",
      "weighted avg       0.79      0.79      0.79      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cvkO7-lc-SxP",
   "metadata": {
    "id": "cvkO7-lc-SxP"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 1000, random_state = 0)\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred_logreg = logreg.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "p7BrOO6Z-eQn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7BrOO6Z-eQn",
    "outputId": "cb82ea05-46a2-4aa6-9c50-c589aa079870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       576\n",
      "           1       0.83      0.79      0.81       576\n",
      "\n",
      "    accuracy                           0.81      1152\n",
      "   macro avg       0.82      0.81      0.81      1152\n",
      "weighted avg       0.82      0.81      0.81      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f433bc-717b-4d11-9df9-65b894ad7b26",
   "metadata": {
    "id": "95f433bc-717b-4d11-9df9-65b894ad7b26"
   },
   "source": [
    "**Вывод:** результат 3 итерации в случае леса хуже, чем на второй, хотя используется более сложная модель для получения эмбеддингов. Однако логистическая регрессия показала себя немного лучше, чем лес."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9e012-befb-4e63-afae-ac8643d0f481",
   "metadata": {
    "id": "c8a9e012-befb-4e63-afae-ac8643d0f481"
   },
   "source": [
    "#### 4 попытка - препроцессинг как во 2 итерации, но использование catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115ee93-4292-4255-9d03-0892b1084f72",
   "metadata": {
    "id": "1115ee93-4292-4255-9d03-0892b1084f72"
   },
   "outputs": [],
   "source": [
    "# параметр verbose - для контроля количества вывода информации\n",
    "# catboost выводит излишне много текста во время обучения\n",
    "boosting = catboost.CatBoostClassifier(verbose=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99339f-a980-4ce3-a90c-98aa9d83ea6e",
   "metadata": {
    "id": "5e99339f-a980-4ce3-a90c-98aa9d83ea6e"
   },
   "outputs": [],
   "source": [
    "x_final_2 = pd.concat((x_final,\n",
    "           x_final.apply(lambda x: len(x))\n",
    "          ), axis=1\n",
    "         )\n",
    "x_final_2.columns = ['text', 'num_words']\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_final_2, train.is_fake, stratify=train.is_fake, test_size = 0.2, random_state = 0)\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "\n",
    "# кодирую фичи через tfidf и сразу же конкатенирую с фичой \"количество слов\"\n",
    "x_train = np.concatenate((tfidf.fit_transform(x_train['text']).toarray(),\n",
    "                          x_train.num_words.values[:, np.newaxis]),\n",
    "                         axis=1)\n",
    "\n",
    "x_val = np.concatenate((tfidf.transform(x_val['text']).toarray(),\n",
    "                        x_val.num_words.values[:, np.newaxis]),\n",
    "                       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a3fdf-632b-41c0-a791-01b627fb088e",
   "metadata": {
    "id": "651a3fdf-632b-41c0-a791-01b627fb088e",
    "outputId": "904fc9a4-b41e-4a06-e9cb-1ff9f0e1033e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.019778\n",
      "0:\tlearn: 0.6888867\ttotal: 235ms\tremaining: 3m 54s\n",
      "100:\tlearn: 0.5373584\ttotal: 7.1s\tremaining: 1m 3s\n",
      "200:\tlearn: 0.5048742\ttotal: 14.2s\tremaining: 56.6s\n",
      "300:\tlearn: 0.4827345\ttotal: 21.4s\tremaining: 49.6s\n",
      "400:\tlearn: 0.4656508\ttotal: 28.5s\tremaining: 42.6s\n",
      "500:\tlearn: 0.4459051\ttotal: 35.7s\tremaining: 35.5s\n",
      "600:\tlearn: 0.4279792\ttotal: 42.7s\tremaining: 28.4s\n",
      "700:\tlearn: 0.4117548\ttotal: 49.8s\tremaining: 21.3s\n",
      "800:\tlearn: 0.3974354\ttotal: 56.7s\tremaining: 14.1s\n",
      "900:\tlearn: 0.3853614\ttotal: 1m 3s\tremaining: 6.99s\n",
      "999:\tlearn: 0.3747131\ttotal: 1m 10s\tremaining: 0us\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81       576\n",
      "           1       0.84      0.73      0.78       576\n",
      "\n",
      "    accuracy                           0.79      1152\n",
      "   macro avg       0.80      0.79      0.79      1152\n",
      "weighted avg       0.80      0.79      0.79      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosting.fit(x_train, y_train)\n",
    "y_pred = boosting.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35e672-730c-448f-a6b3-ef8c426ce896",
   "metadata": {
    "id": "9f35e672-730c-448f-a6b3-ef8c426ce896"
   },
   "source": [
    "Ничего не дало. Используем 2 итерацию, но еще проведём дополнительный GridSearch для выявления лучших параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ohuxSgyK2iCH",
   "metadata": {
    "id": "ohuxSgyK2iCH"
   },
   "source": [
    "#### 5 попытка - использовать более тяжелый Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "Ji72Bvou2nPZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ji72Bvou2nPZ",
    "outputId": "6ce6c89d-2876-4b51-b4b8-fce887e7e435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "Cloning into 'rubert-base-cased-sentence'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 33 (delta 12), reused 0 (delta 0)\u001b[K\n",
      "Unpacking objects: 100% (33/33), done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs clone https://huggingface.co/DeepPavlov/rubert-base-cased-sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sgwC8PFj3Tge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgwC8PFj3Tge",
    "outputId": "7c1c4ff2-e69d-43c2-c470-a161f6843721"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./rubert-base-cased-sentence/\")\n",
    "model = AutoModel.from_pretrained(\"./rubert-base-cased-sentence/\")\n",
    "# model.cuda()  # раскоментируй, если у тебя есть GPU\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(embed_bert_cls('привет мир', model, tokenizer).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "vYSKaegcAi5p",
   "metadata": {
    "id": "vYSKaegcAi5p"
   },
   "outputs": [],
   "source": [
    "# получение эмбеддингов и конкатенация с фичей длины текста\n",
    "x_final_bert = [embed_bert_cls(sentence, model, tokenizer) for sentence in x_final]\n",
    "x_final_bert = np.vstack(x_final_bert)\n",
    "x_final_bert_end = np.hstack((x_final_bert,\n",
    "                              train.title.apply(lambda x: len(x)).values[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "RTW-UFFC_616",
   "metadata": {
    "id": "RTW-UFFC_616"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_final_bert_end, train.is_fake, random_state = 0, stratify = train.is_fake, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "VdWyfkZsAFZr",
   "metadata": {
    "id": "VdWyfkZsAFZr"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 1000, random_state = 0)\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred_logreg = logreg.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ptbY3upAJmI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ptbY3upAJmI",
    "outputId": "f6691902-f603-4142-bcbd-400dcab675db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       576\n",
      "           1       0.85      0.81      0.83       576\n",
      "\n",
      "    accuracy                           0.84      1152\n",
      "   macro avg       0.84      0.84      0.84      1152\n",
      "weighted avg       0.84      0.84      0.84      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y1rNPbK1AN4Y",
   "metadata": {
    "id": "Y1rNPbK1AN4Y"
   },
   "source": [
    "лучший результат! Моё решение - использовать полную модель rubert вместе с logistic regression. Осталось только подобрать наилучшие параметры для логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "z47bmaiZD_Zq",
   "metadata": {
    "id": "z47bmaiZD_Zq"
   },
   "outputs": [],
   "source": [
    "# словарь для поиска по сетке гиперпараметров\n",
    "grid_values = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000], 'max_iter':[1000, 3000]}\n",
    "logreg = LogisticRegression()\n",
    "gridsearch_logreg = GridSearchCV(logreg, grid_values,scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "--qyWZu4FU_r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--qyWZu4FU_r",
    "outputId": "3d5830a8-5bb8-4820-d650-9719c3e92dfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "70 fits failed out of a total of 140.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.74752438        nan 0.74752438        nan 0.75718497\n",
      "        nan 0.75718497        nan 0.78677679        nan 0.78677679\n",
      "        nan 0.82737971        nan 0.82737971        nan 0.847521\n",
      "        nan 0.847521          nan 0.84314881        nan 0.84527246\n",
      "        nan 0.83660492        nan 0.83136986]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'max_iter': [1000, 3000], 'penalty': ['l1', 'l2']},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9258_txFX0j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9258_txFX0j",
    "outputId": "be579d40-bb01-4b3a-cd91-f10310141d5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475209954245283"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_logreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "Wfs22fndFsHL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wfs22fndFsHL",
    "outputId": "53bf2c35-79e7-42bd-9a42-be29104e4712"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'max_iter': 1000, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "Xgt0X3xVGemb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xgt0X3xVGemb",
    "outputId": "6132cbbe-d0d6-4dd8-af11-3055a5fe4a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87       576\n",
      "           1       0.88      0.85      0.87       576\n",
      "\n",
      "    accuracy                           0.87      1152\n",
      "   macro avg       0.87      0.87      0.87      1152\n",
      "weighted avg       0.87      0.87      0.87      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# использую найденные лучшие параметры для логистической регрессии\n",
    "logreg = LogisticRegression(max_iter = 1000, C=10,random_state = 0)\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred_logreg = logreg.predict(x_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NrDIfkZqGn3D",
   "metadata": {
    "id": "NrDIfkZqGn3D"
   },
   "source": [
    "Финальный вариант модели:\n",
    "\n",
    "1) Очистка текста от символов, приведение к нижнему регистру, лемматизация\n",
    "\n",
    "2) Генерация эмбеддингов на основе модели RuBert + дополнительная фича длина символов \n",
    "\n",
    "3) Подаём в логистическую регрессию с гиперапараметрами: максимум_итераций = 1000, регуляризация = 10, порог = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KjLAEMEvHuZw",
   "metadata": {
    "id": "KjLAEMEvHuZw"
   },
   "source": [
    "## Обработка тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6I-lDbwrI4qG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6I-lDbwrI4qG",
    "outputId": "c34840f1-2075-4e45-a034-7b4c7bfc6d19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Роскомнадзор представил реестр сочетаний цвето...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ночью под Минском на президентской горе Белара...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Бывший спичрайтер Юрия Лозы рассказал о трудно...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Сельская церковь, собравшая рекордно низкое ко...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Акции Google рухнули после объявления о переза...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_fake\n",
       "0  Роскомнадзор представил реестр сочетаний цвето...        0\n",
       "1  Ночью под Минском на президентской горе Белара...        0\n",
       "2  Бывший спичрайтер Юрия Лозы рассказал о трудно...        0\n",
       "3  Сельская церковь, собравшая рекордно низкое ко...        0\n",
       "4  Акции Google рухнули после объявления о переза...        0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "oeOz4Uy0H17i",
   "metadata": {
    "id": "oeOz4Uy0H17i"
   },
   "outputs": [],
   "source": [
    "def preprocess_df(df, stopwords, lemmatizer):\n",
    "    '''\n",
    "    df: датафрейм с тренировочными данными без столбца таргетов\n",
    "    stopwords: список стоп-слов\n",
    "    lemmatizer: лемматизатор от pymorphy2\n",
    "\n",
    "    функция проводит предобработку сырого текста\n",
    "    '''\n",
    "    \n",
    "    # приведение к нижнему регистру и удаление стоп-слов\n",
    "    x_filtered = df.title.str.lower().str.split(' ').apply(lambda x: [i for i in x if (i.isalpha() & ~(i in stopwords))])\n",
    "    \n",
    "    # лемматизация\n",
    "    x_lemmatized = x_filtered.apply(lambda x: [(lemmatizer.parse(word)[0]).normal_form for word in x ])\n",
    "\n",
    "    return x_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9_19gqxUI9UJ",
   "metadata": {
    "id": "9_19gqxUI9UJ"
   },
   "outputs": [],
   "source": [
    "prep_text_test = preprocess_df(test, set_stopwords, analyzer).str.join(' ')\n",
    "num_words_test = prep_text_test.apply(lambda x: len(x)).values[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "qhg2z3wEMhjN",
   "metadata": {
    "id": "qhg2z3wEMhjN"
   },
   "outputs": [],
   "source": [
    "embeddings_test = [embed_bert_cls(sentence, model, tokenizer) for sentence in prep_text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "O2TMz0zsM_qC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2TMz0zsM_qC",
    "outputId": "e0a1d048-a0f5-45b6-a017-2707f65604a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 769)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.hstack((embeddings_test,\n",
    "                    num_words_test))\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sjh8EjHyNoFt",
   "metadata": {
    "id": "sjh8EjHyNoFt"
   },
   "outputs": [],
   "source": [
    "test_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "QkwWW-9uNytQ",
   "metadata": {
    "id": "QkwWW-9uNytQ"
   },
   "outputs": [],
   "source": [
    "submission = test_orig.copy()\n",
    "submission['is_fake'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ZbhdoNlhN9ZJ",
   "metadata": {
    "id": "ZbhdoNlhN9ZJ"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../predictions.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "solution_kontur.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
